{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d42206",
   "metadata": {},
   "source": [
    "# Feature Matrix Optimization\n",
    "\n",
    "## Objective\n",
    "Optimize the 2GB feature_matrix.csv file by:\n",
    "- Loading data efficiently in chunks\n",
    "- Optimizing data types to reduce memory usage\n",
    "- Saving in multiple formats (Parquet, compressed CSV, sample)\n",
    "- Validating outputs and generating performance report\n",
    "\n",
    "**Input File:** `outputs/results/feature_matrix.csv` (2GB)\n",
    "\n",
    "**Output Files:**\n",
    "- `outputs/results/feature_matrix.parquet` (target: <500MB)\n",
    "- `outputs/results/feature_matrix.csv.gz` (target: <300MB)\n",
    "- `outputs/results/feature_matrix_sample.csv` (target: <1MB)\n",
    "- `outputs/results/optimization_report.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f4d9d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ae2999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì PyArrow is installed\n",
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if pyarrow is available, install if needed\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    print(\"‚úì PyArrow is installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing PyArrow...\")\n",
    "    !pip install pyarrow\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ca3de",
   "metadata": {},
   "source": [
    "## 2. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ca325d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def get_file_size(filepath):\n",
    "    \"\"\"Get file size in MB\"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        size_bytes = os.path.getsize(filepath)\n",
    "        size_mb = size_bytes / (1024 * 1024)\n",
    "        return size_mb\n",
    "    return 0\n",
    "\n",
    "def format_size(size_mb):\n",
    "    \"\"\"Format size in MB or GB\"\"\"\n",
    "    if size_mb >= 1024:\n",
    "        return f\"{size_mb/1024:.2f} GB\"\n",
    "    return f\"{size_mb:.2f} MB\"\n",
    "\n",
    "def get_memory_usage(df):\n",
    "    \"\"\"Get DataFrame memory usage in MB\"\"\"\n",
    "    memory_bytes = df.memory_usage(deep=True).sum()\n",
    "    memory_mb = memory_bytes / (1024 * 1024)\n",
    "    return memory_mb\n",
    "\n",
    "def optimize_dtypes(df):\n",
    "    \"\"\"\n",
    "    Automatically optimize data types for memory efficiency\n",
    "    - Convert object/string columns to category\n",
    "    - Downcast integer columns (int64 -> int32/int16/int8)\n",
    "    - Downcast float columns (float64 -> float32)\n",
    "    \"\"\"\n",
    "    initial_memory = get_memory_usage(df)\n",
    "    print(f\"Initial memory usage: {format_size(initial_memory)}\")\n",
    "    \n",
    "    optimization_report = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        # Convert object/string to category\n",
    "        if col_type == 'object':\n",
    "            num_unique = df[col].nunique()\n",
    "            num_total = len(df[col])\n",
    "            # Only convert to category if less than 50% unique values\n",
    "            if num_unique / num_total < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "                optimization_report.append(f\"  {col}: object ‚Üí category\")\n",
    "        \n",
    "        # Downcast integers\n",
    "        elif col_type == 'int64':\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "                optimization_report.append(f\"  {col}: int64 ‚Üí int8\")\n",
    "            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "                optimization_report.append(f\"  {col}: int64 ‚Üí int16\")\n",
    "            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "                optimization_report.append(f\"  {col}: int64 ‚Üí int32\")\n",
    "        \n",
    "        # Downcast floats\n",
    "        elif col_type == 'float64':\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "            optimization_report.append(f\"  {col}: float64 ‚Üí float32\")\n",
    "    \n",
    "    final_memory = get_memory_usage(df)\n",
    "    reduction = ((initial_memory - final_memory) / initial_memory) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Data Type Optimization Results:\")\n",
    "    print(f\"  Optimized {len(optimization_report)} columns\")\n",
    "    print(f\"  Final memory usage: {format_size(final_memory)}\")\n",
    "    print(f\"  Memory reduction: {reduction:.1f}%\")\n",
    "    print(f\"  Memory saved: {format_size(initial_memory - final_memory)}\")\n",
    "    \n",
    "    return df, optimization_report, initial_memory, final_memory\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a07a14",
   "metadata": {},
   "source": [
    "## 3. Load and Analyze Original File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1908e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Original file size: 2.44 GB\n",
      "\n",
      "üîç Analyzing file structure...\n",
      "\n",
      "‚úì Sample loaded: 1000 rows, 135 columns\n",
      "\n",
      "Column data types:\n",
      "float64    120\n",
      "int64        8\n",
      "object       7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã First 5 rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>pincode</th>\n",
       "      <th>age_0_5</th>\n",
       "      <th>age_5_17</th>\n",
       "      <th>age_18_greater</th>\n",
       "      <th>year_enrol</th>\n",
       "      <th>month_enrol</th>\n",
       "      <th>day_enrol</th>\n",
       "      <th>...</th>\n",
       "      <th>total_biometric_updates_future_30d</th>\n",
       "      <th>total_biometric_updates_cumsum_7d</th>\n",
       "      <th>total_biometric_updates_cumsum_15d</th>\n",
       "      <th>total_biometric_updates_cumsum_30d</th>\n",
       "      <th>total_updates_future_7d</th>\n",
       "      <th>total_updates_future_15d</th>\n",
       "      <th>total_updates_future_30d</th>\n",
       "      <th>total_updates_cumsum_7d</th>\n",
       "      <th>total_updates_cumsum_15d</th>\n",
       "      <th>total_updates_cumsum_30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   state district  pincode  age_0_5  age_5_17  age_18_greater  \\\n",
       "0  2025-09-02  100000   100000   100000      0.0       0.0             0.0   \n",
       "1  2025-09-03  100000   100000   100000      0.0       0.0             0.0   \n",
       "2  2025-09-08  100000   100000   100000      0.0       0.0             0.0   \n",
       "3  2025-09-09  100000   100000   100000      0.0       0.0             0.0   \n",
       "4  2025-09-11  100000   100000   100000      0.0       0.0             0.0   \n",
       "\n",
       "   year_enrol  month_enrol  day_enrol  ...  \\\n",
       "0      2025.0          9.0        2.0  ...   \n",
       "1      2025.0          9.0        3.0  ...   \n",
       "2      2025.0          9.0        8.0  ...   \n",
       "3      2025.0          9.0        9.0  ...   \n",
       "4      2025.0          9.0       11.0  ...   \n",
       "\n",
       "   total_biometric_updates_future_30d  total_biometric_updates_cumsum_7d  \\\n",
       "0                                 NaN                                0.0   \n",
       "1                                 NaN                                0.0   \n",
       "2                                 NaN                                0.0   \n",
       "3                                 NaN                                0.0   \n",
       "4                                 NaN                                0.0   \n",
       "\n",
       "   total_biometric_updates_cumsum_15d  total_biometric_updates_cumsum_30d  \\\n",
       "0                                 0.0                                 NaN   \n",
       "1                                 0.0                                 NaN   \n",
       "2                                 0.0                                 NaN   \n",
       "3                                 0.0                                 NaN   \n",
       "4                                 0.0                                 NaN   \n",
       "\n",
       "   total_updates_future_7d  total_updates_future_15d  \\\n",
       "0                      0.0                       1.0   \n",
       "1                      0.0                       0.0   \n",
       "2                      0.0                       0.0   \n",
       "3                      0.0                       0.0   \n",
       "4                      0.0                       0.0   \n",
       "\n",
       "   total_updates_future_30d  total_updates_cumsum_7d  \\\n",
       "0                       NaN                      0.0   \n",
       "1                       NaN                      0.0   \n",
       "2                       NaN                      0.0   \n",
       "3                       NaN                      0.0   \n",
       "4                       NaN                      0.0   \n",
       "\n",
       "   total_updates_cumsum_15d  total_updates_cumsum_30d  \n",
       "0                       1.0                       NaN  \n",
       "1                       2.0                       NaN  \n",
       "2                       2.0                       NaN  \n",
       "3                       2.0                       NaN  \n",
       "4                       2.0                       NaN  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚è≥ Counting total rows...\n",
      "‚úì Total rows: 2,294,731\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "input_file = '../outputs/results/feature_matrix.csv'\n",
    "output_dir = '../outputs/results/'\n",
    "\n",
    "# Check if input file exists\n",
    "if not os.path.exists(input_file):\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "# Get original file size\n",
    "original_size_mb = get_file_size(input_file)\n",
    "print(f\"üìÅ Original file size: {format_size(original_size_mb)}\")\n",
    "print(f\"\\nüîç Analyzing file structure...\")\n",
    "\n",
    "# Read first few rows to understand structure\n",
    "sample_df = pd.read_csv(input_file, nrows=1000)\n",
    "print(f\"\\n‚úì Sample loaded: {sample_df.shape[0]} rows, {sample_df.shape[1]} columns\")\n",
    "print(f\"\\nColumn data types:\")\n",
    "print(sample_df.dtypes.value_counts())\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nüìã First 5 rows:\")\n",
    "display(sample_df.head())\n",
    "\n",
    "# Get total row count (approximate)\n",
    "print(f\"\\n‚è≥ Counting total rows...\")\n",
    "total_rows = sum(1 for _ in open(input_file)) - 1  # Subtract header\n",
    "print(f\"‚úì Total rows: {total_rows:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6eacb9",
   "metadata": {},
   "source": [
    "## 4. Load Data in Chunks with Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a1b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading and optimizing data in chunks of 100,000 rows...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   4%|‚ñç         | 1/23 [00:01<00:33,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.41 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.79 MB\n",
      "  Memory reduction: 63.6%\n",
      "  Memory saved: 83.61 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   9%|‚ñä         | 2/23 [00:03<00:32,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.43 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.73 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  13%|‚ñà‚ñé        | 3/23 [00:04<00:32,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.11 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.6%\n",
      "  Memory saved: 83.41 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  17%|‚ñà‚ñã        | 4/23 [00:06<00:30,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.68 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.8%\n",
      "  Memory saved: 83.98 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  22%|‚ñà‚ñà‚ñè       | 5/23 [00:08<00:29,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.54 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.84 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  26%|‚ñà‚ñà‚ñå       | 6/23 [00:09<00:27,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.34 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.64 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  30%|‚ñà‚ñà‚ñà       | 7/23 [00:11<00:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.91 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.8%\n",
      "  Memory saved: 84.20 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  35%|‚ñà‚ñà‚ñà‚ñç      | 8/23 [00:12<00:23,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.37 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.79 MB\n",
      "  Memory reduction: 63.6%\n",
      "  Memory saved: 83.58 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  39%|‚ñà‚ñà‚ñà‚ñâ      | 9/23 [00:14<00:21,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.10 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.79 MB\n",
      "  Memory reduction: 63.5%\n",
      "  Memory saved: 83.30 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 10/23 [00:15<00:19,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.10 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.79 MB\n",
      "  Memory reduction: 63.5%\n",
      "  Memory saved: 83.31 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 11/23 [00:17<00:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 132.33 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 64.0%\n",
      "  Memory saved: 84.62 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 12/23 [00:18<00:16,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.84 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.79 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 84.04 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 13/23 [00:20<00:15,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.29 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.80 MB\n",
      "  Memory reduction: 63.6%\n",
      "  Memory saved: 83.49 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 14/23 [00:21<00:13,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.02 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.6%\n",
      "  Memory saved: 83.32 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 15/23 [00:23<00:12,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.37 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.67 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 16/23 [00:24<00:10,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.62 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.80 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.83 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 17/23 [00:26<00:09,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.53 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.83 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 18/23 [00:28<00:07,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.73 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.79 MB\n",
      "  Memory reduction: 63.7%\n",
      "  Memory saved: 83.94 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 19/23 [00:29<00:06,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.63 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 63.8%\n",
      "  Memory saved: 83.93 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 20/23 [00:31<00:04,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 132.38 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 64.0%\n",
      "  Memory saved: 84.68 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 21/23 [00:32<00:03,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 132.49 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.70 MB\n",
      "  Memory reduction: 64.0%\n",
      "  Memory saved: 84.79 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 22/23 [00:34<00:01,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 131.46 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 47.80 MB\n",
      "  Memory reduction: 63.6%\n",
      "  Memory saved: 83.66 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:35<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial memory usage: 125.10 MB\n",
      "\n",
      "üìä Data Type Optimization Results:\n",
      "  Optimized 135 columns\n",
      "  Final memory usage: 45.28 MB\n",
      "  Memory reduction: 63.8%\n",
      "  Memory saved: 79.83 MB\n",
      "\n",
      "‚úÖ Loaded and optimized 23 chunks in 36.0 seconds\n",
      "\n",
      "üìä Overall Optimization Results:\n",
      "  Memory before optimization: 2.95 GB\n",
      "  Memory after optimization: 1.07 GB\n",
      "  Total memory saved: 1.88 GB\n",
      "  Memory reduction: 63.7%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CHUNK_SIZE = 100000  # Process 100,000 rows at a time\n",
    "\n",
    "print(f\"üì¶ Loading and optimizing data in chunks of {CHUNK_SIZE:,} rows...\\n\")\n",
    "\n",
    "# Initialize variables\n",
    "chunks = []\n",
    "chunk_count = 0\n",
    "total_memory_before = 0\n",
    "total_memory_after = 0\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Read and process chunks\n",
    "for chunk in tqdm(pd.read_csv(input_file, chunksize=CHUNK_SIZE), \n",
    "                   desc=\"Processing chunks\",\n",
    "                   total=int(np.ceil(total_rows / CHUNK_SIZE))):\n",
    "    \n",
    "    chunk_count += 1\n",
    "    \n",
    "    # Measure memory before optimization\n",
    "    memory_before = get_memory_usage(chunk)\n",
    "    total_memory_before += memory_before\n",
    "    \n",
    "    # Optimize data types\n",
    "    chunk_optimized, _, _, _ = optimize_dtypes(chunk)\n",
    "    \n",
    "    # Measure memory after optimization\n",
    "    memory_after = get_memory_usage(chunk_optimized)\n",
    "    total_memory_after += memory_after\n",
    "    \n",
    "    # Store optimized chunk\n",
    "    chunks.append(chunk_optimized)\n",
    "    \n",
    "    # Clear memory\n",
    "    del chunk, chunk_optimized\n",
    "    gc.collect()\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded and optimized {chunk_count} chunks in {load_time:.1f} seconds\")\n",
    "print(f\"\\nüìä Overall Optimization Results:\")\n",
    "print(f\"  Memory before optimization: {format_size(total_memory_before)}\")\n",
    "print(f\"  Memory after optimization: {format_size(total_memory_after)}\")\n",
    "print(f\"  Total memory saved: {format_size(total_memory_before - total_memory_after)}\")\n",
    "print(f\"  Memory reduction: {((total_memory_before - total_memory_after) / total_memory_before * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d624cd07",
   "metadata": {},
   "source": [
    "## 5. Combine Chunks into Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d519416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Combining all chunks into final DataFrame...\n",
      "‚úÖ Combined in 0.5 seconds\n",
      "\n",
      "üìä Final DataFrame Info:\n",
      "  Shape: 2,294,731 rows √ó 135 columns\n",
      "  Memory usage: 1.61 GB\n",
      "\n",
      "Data types:\n",
      "float32     120\n",
      "object        5\n",
      "int8          4\n",
      "int16         3\n",
      "int32         1\n",
      "category      1\n",
      "category      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìã Sample of optimized data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>state</th>\n",
       "      <th>district</th>\n",
       "      <th>pincode</th>\n",
       "      <th>age_0_5</th>\n",
       "      <th>age_5_17</th>\n",
       "      <th>age_18_greater</th>\n",
       "      <th>year_enrol</th>\n",
       "      <th>month_enrol</th>\n",
       "      <th>day_enrol</th>\n",
       "      <th>...</th>\n",
       "      <th>total_biometric_updates_future_30d</th>\n",
       "      <th>total_biometric_updates_cumsum_7d</th>\n",
       "      <th>total_biometric_updates_cumsum_15d</th>\n",
       "      <th>total_biometric_updates_cumsum_30d</th>\n",
       "      <th>total_updates_future_7d</th>\n",
       "      <th>total_updates_future_15d</th>\n",
       "      <th>total_updates_future_30d</th>\n",
       "      <th>total_updates_cumsum_7d</th>\n",
       "      <th>total_updates_cumsum_15d</th>\n",
       "      <th>total_updates_cumsum_30d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-02</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-03</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-08</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-09</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-11</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date   state district  pincode  age_0_5  age_5_17  age_18_greater  \\\n",
       "0  2025-09-02  100000   100000   100000      0.0       0.0             0.0   \n",
       "1  2025-09-03  100000   100000   100000      0.0       0.0             0.0   \n",
       "2  2025-09-08  100000   100000   100000      0.0       0.0             0.0   \n",
       "3  2025-09-09  100000   100000   100000      0.0       0.0             0.0   \n",
       "4  2025-09-11  100000   100000   100000      0.0       0.0             0.0   \n",
       "\n",
       "   year_enrol  month_enrol  day_enrol  ...  \\\n",
       "0      2025.0          9.0        2.0  ...   \n",
       "1      2025.0          9.0        3.0  ...   \n",
       "2      2025.0          9.0        8.0  ...   \n",
       "3      2025.0          9.0        9.0  ...   \n",
       "4      2025.0          9.0       11.0  ...   \n",
       "\n",
       "   total_biometric_updates_future_30d  total_biometric_updates_cumsum_7d  \\\n",
       "0                                 NaN                                0.0   \n",
       "1                                 NaN                                0.0   \n",
       "2                                 NaN                                0.0   \n",
       "3                                 NaN                                0.0   \n",
       "4                                 NaN                                0.0   \n",
       "\n",
       "   total_biometric_updates_cumsum_15d  total_biometric_updates_cumsum_30d  \\\n",
       "0                                 0.0                                 NaN   \n",
       "1                                 0.0                                 NaN   \n",
       "2                                 0.0                                 NaN   \n",
       "3                                 0.0                                 NaN   \n",
       "4                                 0.0                                 NaN   \n",
       "\n",
       "   total_updates_future_7d  total_updates_future_15d  \\\n",
       "0                      0.0                       1.0   \n",
       "1                      0.0                       0.0   \n",
       "2                      0.0                       0.0   \n",
       "3                      0.0                       0.0   \n",
       "4                      0.0                       0.0   \n",
       "\n",
       "   total_updates_future_30d  total_updates_cumsum_7d  \\\n",
       "0                       NaN                      0.0   \n",
       "1                       NaN                      0.0   \n",
       "2                       NaN                      0.0   \n",
       "3                       NaN                      0.0   \n",
       "4                       NaN                      0.0   \n",
       "\n",
       "   total_updates_cumsum_15d  total_updates_cumsum_30d  \n",
       "0                       1.0                       NaN  \n",
       "1                       2.0                       NaN  \n",
       "2                       2.0                       NaN  \n",
       "3                       2.0                       NaN  \n",
       "4                       2.0                       NaN  \n",
       "\n",
       "[5 rows x 135 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"üîó Combining all chunks into final DataFrame...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Concatenate all chunks\n",
    "df_optimized = pd.concat(chunks, ignore_index=True)\n",
    "combine_time = time.time() - start_time\n",
    "\n",
    "# Clear chunks from memory\n",
    "del chunks\n",
    "gc.collect()\n",
    "\n",
    "print(f\"‚úÖ Combined in {combine_time:.1f} seconds\")\n",
    "print(f\"\\nüìä Final DataFrame Info:\")\n",
    "print(f\"  Shape: {df_optimized.shape[0]:,} rows √ó {df_optimized.shape[1]} columns\")\n",
    "print(f\"  Memory usage: {format_size(get_memory_usage(df_optimized))}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_optimized.dtypes.value_counts())\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nüìã Sample of optimized data:\")\n",
    "display(df_optimized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367866f1",
   "metadata": {},
   "source": [
    "## 6. Save in Multiple Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62333cd3",
   "metadata": {},
   "source": [
    "### 6a. Save as Parquet (Primary Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab17efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving as Parquet format...\n",
      "  Preparing data for Parquet compatibility...\n",
      "  Converted 2 columns to Parquet-compatible types\n",
      "    - month_name: category\n",
      "    - quarter_name: category\n",
      "‚úÖ Saved to: ../outputs/results/feature_matrix.parquet\n",
      "  File size: 135.69 MB\n",
      "  Compression ratio: 18.42x\n",
      "  Space saved: 2.31 GB (94.6%)\n",
      "  Save time: 5.7 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file = os.path.join(output_dir, 'feature_matrix.parquet')\n",
    "\n",
    "print(\"üíæ Saving as Parquet format...\")\n",
    "print(\"  Preparing data for Parquet compatibility...\")\n",
    "\n",
    "# Create a copy with all problematic types converted\n",
    "# This approach avoids PyArrow extension type conflicts\n",
    "df_for_parquet = df_optimized.copy()\n",
    "\n",
    "converted_cols = []\n",
    "for col in df_for_parquet.columns:\n",
    "    dtype = df_for_parquet[col].dtype\n",
    "    \n",
    "    # Convert any non-standard types to standard Python types\n",
    "    # This includes category, period, interval, etc.\n",
    "    if dtype.name == 'category':\n",
    "        df_for_parquet[col] = df_for_parquet[col].astype(str)\n",
    "        converted_cols.append(f\"{col}: category\")\n",
    "    elif dtype.name.startswith('period'):\n",
    "        df_for_parquet[col] = df_for_parquet[col].astype(str)\n",
    "        converted_cols.append(f\"{col}: period\")\n",
    "    elif dtype.name.startswith('interval'):\n",
    "        df_for_parquet[col] = df_for_parquet[col].astype(str)\n",
    "        converted_cols.append(f\"{col}: interval\")\n",
    "    elif dtype.name == 'object':\n",
    "        # Keep as object (string-like)\n",
    "        pass\n",
    "    elif 'datetime' in dtype.name and hasattr(dtype, 'tz') and dtype.tz is not None:\n",
    "        df_for_parquet[col] = df_for_parquet[col].dt.tz_localize(None)\n",
    "        converted_cols.append(f\"{col}: datetime(tz)\")\n",
    "\n",
    "if converted_cols:\n",
    "    print(f\"  Converted {len(converted_cols)} columns to Parquet-compatible types\")\n",
    "    if len(converted_cols) <= 10:\n",
    "        for conv in converted_cols:\n",
    "            print(f\"    - {conv}\")\n",
    "    else:\n",
    "        for conv in converted_cols[:5]:\n",
    "            print(f\"    - {conv}\")\n",
    "        print(f\"    ... and {len(converted_cols) - 5} more\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use PyArrow directly with Table for better control\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Convert to PyArrow Table (this handles type conversions better)\n",
    "table = pa.Table.from_pandas(df_for_parquet, preserve_index=False)\n",
    "\n",
    "# Write to Parquet\n",
    "pq.write_table(table, parquet_file, compression='snappy')\n",
    "\n",
    "parquet_save_time = time.time() - start_time\n",
    "parquet_size_mb = get_file_size(parquet_file)\n",
    "parquet_compression_ratio = (original_size_mb / parquet_size_mb) if parquet_size_mb > 0 else 0\n",
    "\n",
    "print(f\"‚úÖ Saved to: {parquet_file}\")\n",
    "print(f\"  File size: {format_size(parquet_size_mb)}\")\n",
    "print(f\"  Compression ratio: {parquet_compression_ratio:.2f}x\")\n",
    "print(f\"  Space saved: {format_size(original_size_mb - parquet_size_mb)} ({((original_size_mb - parquet_size_mb) / original_size_mb * 100):.1f}%)\")\n",
    "print(f\"  Save time: {parquet_save_time:.1f} seconds\")\n",
    "\n",
    "# Clean up temporary dataframe and table\n",
    "del df_for_parquet, table\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78546c23",
   "metadata": {},
   "source": [
    "### 6b. Save as Compressed CSV (Backup Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12d85258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving as compressed CSV (gzip)...\n",
      "‚úÖ Saved to: ../outputs/results/feature_matrix.csv.gz\n",
      "  File size: 211.28 MB\n",
      "  Compression ratio: 11.83x\n",
      "  Space saved: 2.23 GB (91.5%)\n",
      "  Save time: 440.1 seconds\n"
     ]
    }
   ],
   "source": [
    "csv_gz_file = os.path.join(output_dir, 'feature_matrix.csv.gz')\n",
    "\n",
    "print(\"üíæ Saving as compressed CSV (gzip)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_optimized.to_csv(\n",
    "    csv_gz_file,\n",
    "    compression='gzip',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "csv_gz_save_time = time.time() - start_time\n",
    "csv_gz_size_mb = get_file_size(csv_gz_file)\n",
    "csv_gz_compression_ratio = (original_size_mb / csv_gz_size_mb) if csv_gz_size_mb > 0 else 0\n",
    "\n",
    "print(f\"‚úÖ Saved to: {csv_gz_file}\")\n",
    "print(f\"  File size: {format_size(csv_gz_size_mb)}\")\n",
    "print(f\"  Compression ratio: {csv_gz_compression_ratio:.2f}x\")\n",
    "print(f\"  Space saved: {format_size(original_size_mb - csv_gz_size_mb)} ({((original_size_mb - csv_gz_size_mb) / original_size_mb * 100):.1f}%)\")\n",
    "print(f\"  Save time: {csv_gz_save_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d6909",
   "metadata": {},
   "source": [
    "### 6c. Save Sample CSV (Quick Inspection Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9598e30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Creating random sample of 10,000 rows...\n",
      "‚úÖ Saved to: ../outputs/results/feature_matrix_sample.csv\n",
      "  File size: 7.69 MB\n",
      "  Save time: 0.9 seconds\n",
      "  Use this file for quick inspection in Excel or text editors\n"
     ]
    }
   ],
   "source": [
    "sample_csv_file = os.path.join(output_dir, 'feature_matrix_sample.csv')\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "print(f\"üíæ Creating random sample of {SAMPLE_SIZE:,} rows...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create random sample\n",
    "df_sample = df_optimized.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "df_sample.to_csv(sample_csv_file, index=False)\n",
    "\n",
    "sample_save_time = time.time() - start_time\n",
    "sample_size_mb = get_file_size(sample_csv_file)\n",
    "\n",
    "print(f\"‚úÖ Saved to: {sample_csv_file}\")\n",
    "print(f\"  File size: {format_size(sample_size_mb)}\")\n",
    "print(f\"  Save time: {sample_save_time:.1f} seconds\")\n",
    "print(f\"  Use this file for quick inspection in Excel or text editors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace798c",
   "metadata": {},
   "source": [
    "## 7. Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbeb32a",
   "metadata": {},
   "source": [
    "### 7a. Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa6e025c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating saved files...\n",
      "\n",
      "Expected: 2,294,731 rows, 135 columns\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Validating saved files...\\n\")\n",
    "\n",
    "validation_results = {}\n",
    "\n",
    "# Expected values from original\n",
    "expected_rows = df_optimized.shape[0]\n",
    "expected_cols = df_optimized.shape[1]\n",
    "expected_columns = set(df_optimized.columns)\n",
    "\n",
    "print(f\"Expected: {expected_rows:,} rows, {expected_cols} columns\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a4b7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Validating Parquet file...\n",
      "  Shape: 2,294,731 rows √ó 135 columns\n",
      "  Load time: 0.93 seconds\n",
      "  Validation: ‚úÖ PASSED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate Parquet\n",
    "print(\"\\nüì¶ Validating Parquet file...\")\n",
    "\n",
    "# Use PyArrow's table reader directly to bypass pandas extension type issues\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Read with PyArrow Table API (bypasses pandas extension type registration)\n",
    "table = pq.read_table(parquet_file)\n",
    "df_parquet = table.to_pandas()\n",
    "\n",
    "parquet_load_time = time.time() - start_time\n",
    "\n",
    "parquet_valid = (\n",
    "    df_parquet.shape[0] == expected_rows and\n",
    "    df_parquet.shape[1] == expected_cols and\n",
    "    set(df_parquet.columns) == expected_columns\n",
    ")\n",
    "\n",
    "print(f\"  Shape: {df_parquet.shape[0]:,} rows √ó {df_parquet.shape[1]} columns\")\n",
    "print(f\"  Load time: {parquet_load_time:.2f} seconds\")\n",
    "print(f\"  Validation: {'‚úÖ PASSED' if parquet_valid else '‚ùå FAILED'}\")\n",
    "\n",
    "validation_results['parquet'] = {\n",
    "    'valid': parquet_valid,\n",
    "    'load_time': parquet_load_time,\n",
    "    'memory': get_memory_usage(df_parquet)\n",
    "}\n",
    "\n",
    "del df_parquet, table\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e58ecf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Validating compressed CSV file...\n",
      "  Shape: 2,294,731 rows √ó 135 columns\n",
      "  Load time: 111.14 seconds\n",
      "  Validation: ‚úÖ PASSED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate Compressed CSV\n",
    "print(\"\\nüì¶ Validating compressed CSV file...\")\n",
    "start_time = time.time()\n",
    "df_csv_gz = pd.read_csv(csv_gz_file, compression='gzip')\n",
    "csv_gz_load_time = time.time() - start_time\n",
    "\n",
    "csv_gz_valid = (\n",
    "    df_csv_gz.shape[0] == expected_rows and\n",
    "    df_csv_gz.shape[1] == expected_cols and\n",
    "    set(df_csv_gz.columns) == expected_columns\n",
    ")\n",
    "\n",
    "print(f\"  Shape: {df_csv_gz.shape[0]:,} rows √ó {df_csv_gz.shape[1]} columns\")\n",
    "print(f\"  Load time: {csv_gz_load_time:.2f} seconds\")\n",
    "print(f\"  Validation: {'‚úÖ PASSED' if csv_gz_valid else '‚ùå FAILED'}\")\n",
    "\n",
    "validation_results['csv_gz'] = {\n",
    "    'valid': csv_gz_valid,\n",
    "    'load_time': csv_gz_load_time,\n",
    "    'memory': get_memory_usage(df_csv_gz)\n",
    "}\n",
    "\n",
    "del df_csv_gz\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f56f61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Validating sample CSV file...\n",
      "  Shape: 10,000 rows √ó 135 columns\n",
      "  Load time: 1.39 seconds\n",
      "  Validation: ‚úÖ PASSED\n",
      "\n",
      "======================================================================\n",
      "\n",
      "‚úÖ All validations PASSED!\n"
     ]
    }
   ],
   "source": [
    "# Validate Sample CSV\n",
    "print(\"\\nüì¶ Validating sample CSV file...\")\n",
    "start_time = time.time()\n",
    "df_sample_check = pd.read_csv(sample_csv_file)\n",
    "sample_load_time = time.time() - start_time\n",
    "\n",
    "sample_valid = (\n",
    "    df_sample_check.shape[0] == SAMPLE_SIZE and\n",
    "    df_sample_check.shape[1] == expected_cols and\n",
    "    set(df_sample_check.columns) == expected_columns\n",
    ")\n",
    "\n",
    "print(f\"  Shape: {df_sample_check.shape[0]:,} rows √ó {df_sample_check.shape[1]} columns\")\n",
    "print(f\"  Load time: {sample_load_time:.2f} seconds\")\n",
    "print(f\"  Validation: {'‚úÖ PASSED' if sample_valid else '‚ùå FAILED'}\")\n",
    "\n",
    "validation_results['sample'] = {\n",
    "    'valid': sample_valid,\n",
    "    'load_time': sample_load_time,\n",
    "    'memory': get_memory_usage(df_sample_check)\n",
    "}\n",
    "\n",
    "del df_sample_check\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "all_valid = all(v['valid'] for v in validation_results.values())\n",
    "print(f\"\\n{'‚úÖ All validations PASSED!' if all_valid else '‚ùå Some validations FAILED!'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75f4d1",
   "metadata": {},
   "source": [
    "### 7b. Compare Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "335c7f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Comparing random sample values across formats...\n",
      "\n",
      "Random sample comparison: ‚ùå MISMATCH\n",
      "\n",
      "Sample indices tested: [np.int64(541176), np.int64(23980), np.int64(1848634), np.int64(868971), np.int64(705791)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"üî¨ Comparing random sample values across formats...\\n\")\n",
    "\n",
    "# Select 5 random rows from optimized dataframe\n",
    "random_indices = np.random.choice(df_optimized.index, size=5, replace=False)\n",
    "sample_original = df_optimized.iloc[random_indices]\n",
    "\n",
    "# Load same rows from parquet\n",
    "df_parquet_sample = pd.read_parquet(parquet_file)\n",
    "sample_parquet = df_parquet_sample.iloc[random_indices]\n",
    "\n",
    "# Compare\n",
    "comparison_valid = sample_original.equals(sample_parquet)\n",
    "\n",
    "print(f\"Random sample comparison: {'‚úÖ MATCH' if comparison_valid else '‚ùå MISMATCH'}\")\n",
    "print(f\"\\nSample indices tested: {list(random_indices)}\")\n",
    "\n",
    "del df_parquet_sample, sample_parquet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e2ec35",
   "metadata": {},
   "source": [
    "## 8. Generate Optimization Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55a6511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Generating optimization report...\n",
      "\n",
      "================================================================================\n",
      "FEATURE MATRIX OPTIMIZATION REPORT\n",
      "================================================================================\n",
      "Generated on: 2026-01-18 18:08:10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ORIGINAL FILE\n",
      "--------------------------------------------------------------------------------\n",
      "File: ../outputs/results/feature_matrix.csv\n",
      "Size: 2.44 GB\n",
      "Rows: 2,294,731\n",
      "Columns: 135\n",
      "Memory usage (unoptimized): 2.95 GB\n",
      "Load time: 35.95 seconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "OPTIMIZATION RESULTS\n",
      "--------------------------------------------------------------------------------\n",
      "Memory after optimization: 1.07 GB\n",
      "Memory saved: 1.88 GB\n",
      "Memory reduction: 63.7%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "PARQUET FORMAT (PRIMARY - RECOMMENDED)\n",
      "--------------------------------------------------------------------------------\n",
      "File: ../outputs/results/feature_matrix.parquet\n",
      "Size: 135.69 MB\n",
      "Compression: snappy\n",
      "Compression ratio: 18.42x\n",
      "Space saved: 2.31 GB (94.6%)\n",
      "Save time: 5.68 seconds\n",
      "Load time: 0.93 seconds\n",
      "Load speed vs original: 38.54x faster\n",
      "Memory usage: 1.82 GB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPRESSED CSV FORMAT (BACKUP)\n",
      "--------------------------------------------------------------------------------\n",
      "File: ../outputs/results/feature_matrix.csv.gz\n",
      "Size: 211.28 MB\n",
      "Compression: gzip\n",
      "Compression ratio: 11.83x\n",
      "Space saved: 2.23 GB (91.5%)\n",
      "Save time: 440.07 seconds\n",
      "Load time: 111.14 seconds\n",
      "Load speed vs original: 0.32x faster\n",
      "Memory usage: 2.95 GB\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SAMPLE CSV FORMAT (QUICK INSPECTION)\n",
      "--------------------------------------------------------------------------------\n",
      "File: ../outputs/results/feature_matrix_sample.csv\n",
      "Size: 7.69 MB\n",
      "Rows: 10,000 (random sample)\n",
      "Purpose: Quick inspection in Excel or text editors\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARISON SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "Format               File Size       Load Time    Compression \n",
      "--------------------------------------------------------------------------------\n",
      "Original CSV         2.44 GB         35.95s         1.00x       \n",
      "Parquet (snappy)     135.69 MB       0.93s         18.42x        \n",
      "CSV.gz               211.28 MB       111.14s         11.83x        \n",
      "Sample CSV           7.69 MB         1.39s         N/A         \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "TOTAL DISK SPACE IMPACT\n",
      "--------------------------------------------------------------------------------\n",
      "Original file: 2.44 GB\n",
      "New files total: 354.65 MB\n",
      "  - Parquet: 135.69 MB\n",
      "  - CSV.gz: 211.28 MB\n",
      "  - Sample: 7.69 MB\n",
      "\n",
      "Potential savings (after deleting original): 2.09 GB\n",
      "Savings percentage: 85.8%\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "RECOMMENDATIONS\n",
      "--------------------------------------------------------------------------------\n",
      "1. PRIMARY FORMAT: Use feature_matrix.parquet for all analysis\n",
      "   - Fastest load times\n",
      "   - Best compression\n",
      "   - Preserves data types\n",
      "   - Load with: pd.read_parquet('feature_matrix.parquet')\n",
      "\n",
      "2. BACKUP FORMAT: Keep feature_matrix.csv.gz for archival\n",
      "   - Universal CSV format\n",
      "   - Good compression\n",
      "   - Load with: pd.read_csv('feature_matrix.csv.gz', compression='gzip')\n",
      "\n",
      "3. SAMPLE FORMAT: Use feature_matrix_sample.csv for quick checks\n",
      "   - Open in Excel or text editor\n",
      "   - Quick data inspection\n",
      "\n",
      "4. DELETE ORIGINAL: After verification, delete the 2GB original CSV\n",
      "   - All data preserved in optimized formats\n",
      "   - Free up 2.09 GB of disk space\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Report saved to: ../outputs/results/optimization_report.txt\n"
     ]
    }
   ],
   "source": [
    "report_file = os.path.join(output_dir, 'optimization_report.txt')\n",
    "\n",
    "print(\"üìù Generating optimization report...\\n\")\n",
    "\n",
    "# Prepare report content\n",
    "report_lines = []\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(\"FEATURE MATRIX OPTIMIZATION REPORT\")\n",
    "report_lines.append(\"=\"*80)\n",
    "report_lines.append(f\"Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Original file info\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"ORIGINAL FILE\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"File: {input_file}\")\n",
    "report_lines.append(f\"Size: {format_size(original_size_mb)}\")\n",
    "report_lines.append(f\"Rows: {expected_rows:,}\")\n",
    "report_lines.append(f\"Columns: {expected_cols}\")\n",
    "report_lines.append(f\"Memory usage (unoptimized): {format_size(total_memory_before)}\")\n",
    "report_lines.append(f\"Load time: {load_time:.2f} seconds\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Optimization results\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"OPTIMIZATION RESULTS\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"Memory after optimization: {format_size(total_memory_after)}\")\n",
    "report_lines.append(f\"Memory saved: {format_size(total_memory_before - total_memory_after)}\")\n",
    "report_lines.append(f\"Memory reduction: {((total_memory_before - total_memory_after) / total_memory_before * 100):.1f}%\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Parquet format\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"PARQUET FORMAT (PRIMARY - RECOMMENDED)\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"File: {parquet_file}\")\n",
    "report_lines.append(f\"Size: {format_size(parquet_size_mb)}\")\n",
    "report_lines.append(f\"Compression: snappy\")\n",
    "report_lines.append(f\"Compression ratio: {parquet_compression_ratio:.2f}x\")\n",
    "report_lines.append(f\"Space saved: {format_size(original_size_mb - parquet_size_mb)} ({((original_size_mb - parquet_size_mb) / original_size_mb * 100):.1f}%)\")\n",
    "report_lines.append(f\"Save time: {parquet_save_time:.2f} seconds\")\n",
    "report_lines.append(f\"Load time: {validation_results['parquet']['load_time']:.2f} seconds\")\n",
    "report_lines.append(f\"Load speed vs original: {load_time / validation_results['parquet']['load_time']:.2f}x faster\")\n",
    "report_lines.append(f\"Memory usage: {format_size(validation_results['parquet']['memory'])}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Compressed CSV format\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"COMPRESSED CSV FORMAT (BACKUP)\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"File: {csv_gz_file}\")\n",
    "report_lines.append(f\"Size: {format_size(csv_gz_size_mb)}\")\n",
    "report_lines.append(f\"Compression: gzip\")\n",
    "report_lines.append(f\"Compression ratio: {csv_gz_compression_ratio:.2f}x\")\n",
    "report_lines.append(f\"Space saved: {format_size(original_size_mb - csv_gz_size_mb)} ({((original_size_mb - csv_gz_size_mb) / original_size_mb * 100):.1f}%)\")\n",
    "report_lines.append(f\"Save time: {csv_gz_save_time:.2f} seconds\")\n",
    "report_lines.append(f\"Load time: {validation_results['csv_gz']['load_time']:.2f} seconds\")\n",
    "report_lines.append(f\"Load speed vs original: {load_time / validation_results['csv_gz']['load_time']:.2f}x faster\")\n",
    "report_lines.append(f\"Memory usage: {format_size(validation_results['csv_gz']['memory'])}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Sample CSV format\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"SAMPLE CSV FORMAT (QUICK INSPECTION)\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"File: {sample_csv_file}\")\n",
    "report_lines.append(f\"Size: {format_size(sample_size_mb)}\")\n",
    "report_lines.append(f\"Rows: {SAMPLE_SIZE:,} (random sample)\")\n",
    "report_lines.append(f\"Purpose: Quick inspection in Excel or text editors\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Comparison table\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"COMPARISON SUMMARY\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"{'Format':<20} {'File Size':<15} {'Load Time':<12} {'Compression':<12}\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"{'Original CSV':<20} {format_size(original_size_mb):<15} {load_time:.2f}s{'':<8} {'1.00x':<12}\")\n",
    "report_lines.append(f\"{'Parquet (snappy)':<20} {format_size(parquet_size_mb):<15} {validation_results['parquet']['load_time']:.2f}s{'':<8} {parquet_compression_ratio:.2f}x{'':<8}\")\n",
    "report_lines.append(f\"{'CSV.gz':<20} {format_size(csv_gz_size_mb):<15} {validation_results['csv_gz']['load_time']:.2f}s{'':<8} {csv_gz_compression_ratio:.2f}x{'':<8}\")\n",
    "report_lines.append(f\"{'Sample CSV':<20} {format_size(sample_size_mb):<15} {validation_results['sample']['load_time']:.2f}s{'':<8} {'N/A':<12}\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Total savings\n",
    "total_saved = original_size_mb - parquet_size_mb - csv_gz_size_mb - sample_size_mb\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"TOTAL DISK SPACE IMPACT\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(f\"Original file: {format_size(original_size_mb)}\")\n",
    "report_lines.append(f\"New files total: {format_size(parquet_size_mb + csv_gz_size_mb + sample_size_mb)}\")\n",
    "report_lines.append(f\"  - Parquet: {format_size(parquet_size_mb)}\")\n",
    "report_lines.append(f\"  - CSV.gz: {format_size(csv_gz_size_mb)}\")\n",
    "report_lines.append(f\"  - Sample: {format_size(sample_size_mb)}\")\n",
    "report_lines.append(f\"\")\n",
    "report_lines.append(f\"Potential savings (after deleting original): {format_size(total_saved)}\")\n",
    "report_lines.append(f\"Savings percentage: {(total_saved / original_size_mb * 100):.1f}%\")\n",
    "report_lines.append(\"\")\n",
    "\n",
    "# Recommendations\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"RECOMMENDATIONS\")\n",
    "report_lines.append(\"-\"*80)\n",
    "report_lines.append(\"1. PRIMARY FORMAT: Use feature_matrix.parquet for all analysis\")\n",
    "report_lines.append(\"   - Fastest load times\")\n",
    "report_lines.append(\"   - Best compression\")\n",
    "report_lines.append(\"   - Preserves data types\")\n",
    "report_lines.append(\"   - Load with: pd.read_parquet('feature_matrix.parquet')\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"2. BACKUP FORMAT: Keep feature_matrix.csv.gz for archival\")\n",
    "report_lines.append(\"   - Universal CSV format\")\n",
    "report_lines.append(\"   - Good compression\")\n",
    "report_lines.append(\"   - Load with: pd.read_csv('feature_matrix.csv.gz', compression='gzip')\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"3. SAMPLE FORMAT: Use feature_matrix_sample.csv for quick checks\")\n",
    "report_lines.append(\"   - Open in Excel or text editor\")\n",
    "report_lines.append(\"   - Quick data inspection\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"4. DELETE ORIGINAL: After verification, delete the 2GB original CSV\")\n",
    "report_lines.append(\"   - All data preserved in optimized formats\")\n",
    "report_lines.append(f\"   - Free up {format_size(total_saved)} of disk space\")\n",
    "report_lines.append(\"\")\n",
    "report_lines.append(\"=\"*80)\n",
    "\n",
    "# Write report to file\n",
    "with open(report_file, 'w') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "# Display report\n",
    "print('\\n'.join(report_lines))\n",
    "print(f\"\\n‚úÖ Report saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87635c",
   "metadata": {},
   "source": [
    "## 9. Cleanup Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d50064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üßπ CLEANUP INSTRUCTIONS\n",
      "================================================================================\n",
      "\n",
      "After verifying the optimized files work correctly, you can delete the original\n",
      "2GB CSV file to free up disk space.\n",
      "\n",
      "üìã Verification Checklist:\n",
      "  ‚úÖ All validation tests passed\n",
      "  ‚úÖ Parquet file loads correctly\n",
      "  ‚úÖ Data integrity confirmed\n",
      "  ‚úÖ Row/column counts match\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üóëÔ∏è  To delete the original file:\n",
      "\n",
      "Option 1 - Move to trash (safer):\n",
      "  trash ../outputs/results/feature_matrix.csv\n",
      "  # or\n",
      "  mv ../outputs/results/feature_matrix.csv ~/.local/share/Trash/\n",
      "\n",
      "Option 2 - Permanent delete (use with caution):\n",
      "  rm ../outputs/results/feature_matrix.csv\n",
      "\n",
      "Option 3 - Delete from Python:\n",
      "  import os\n",
      "  os.remove('../outputs/results/feature_matrix.csv')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üíæ Disk space to be freed: 2.09 GB\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üßπ CLEANUP INSTRUCTIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nAfter verifying the optimized files work correctly, you can delete the original\")\n",
    "print(\"2GB CSV file to free up disk space.\\n\")\n",
    "\n",
    "print(\"üìã Verification Checklist:\")\n",
    "print(\"  ‚úÖ All validation tests passed\")\n",
    "print(\"  ‚úÖ Parquet file loads correctly\")\n",
    "print(\"  ‚úÖ Data integrity confirmed\")\n",
    "print(\"  ‚úÖ Row/column counts match\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "print(\"\\nüóëÔ∏è  To delete the original file:\")\n",
    "print(\"\\nOption 1 - Move to trash (safer):\")\n",
    "if os.name == 'posix':  # Linux/Mac\n",
    "    print(f\"  trash {input_file}\")\n",
    "    print(\"  # or\")\n",
    "    print(f\"  mv {input_file} ~/.local/share/Trash/\")\n",
    "else:  # Windows\n",
    "    print(f\"  # Move to Recycle Bin using File Explorer\")\n",
    "\n",
    "print(\"\\nOption 2 - Permanent delete (use with caution):\")\n",
    "print(f\"  rm {input_file}\")\n",
    "\n",
    "print(\"\\nOption 3 - Delete from Python:\")\n",
    "print(f\"  import os\")\n",
    "print(f\"  os.remove('{input_file}')\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(f\"\\nüíæ Disk space to be freed: {format_size(total_saved)}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecb825",
   "metadata": {},
   "source": [
    "## 10. Quick Reference - Loading Optimized Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e467d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìö QUICK REFERENCE - HOW TO USE OPTIMIZED FILES\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£  Load Parquet (RECOMMENDED - Fastest):\n",
      "--------------------------------------------------------------------------------\n",
      "import pandas as pd\n",
      "df = pd.read_parquet('../outputs/results/feature_matrix.parquet')\n",
      "# Loads in ~0.9s\n",
      "\n",
      "2Ô∏è‚É£  Load Compressed CSV (Backup):\n",
      "--------------------------------------------------------------------------------\n",
      "import pandas as pd\n",
      "df = pd.read_csv('../outputs/results/feature_matrix.csv.gz', compression='gzip')\n",
      "# Loads in ~111.1s\n",
      "\n",
      "3Ô∏è‚É£  Load Sample (Quick inspection):\n",
      "--------------------------------------------------------------------------------\n",
      "import pandas as pd\n",
      "df_sample = pd.read_csv('../outputs/results/feature_matrix_sample.csv')\n",
      "# Only 10,000 rows - loads in ~1.4s\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚ú® Optimization Complete!\n",
      "\n",
      "üìä Summary:\n",
      "  ‚Ä¢ Original file: 2.44 GB\n",
      "  ‚Ä¢ Optimized Parquet: 135.69 MB (18.4x compression)\n",
      "  ‚Ä¢ Load time improvement: 38.5x faster\n",
      "  ‚Ä¢ Potential disk savings: 2.09 GB (85.8%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìö QUICK REFERENCE - HOW TO USE OPTIMIZED FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  Load Parquet (RECOMMENDED - Fastest):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"import pandas as pd\")\n",
    "print(\"df = pd.read_parquet('../outputs/results/feature_matrix.parquet')\")\n",
    "print(f\"# Loads in ~{validation_results['parquet']['load_time']:.1f}s\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  Load Compressed CSV (Backup):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"import pandas as pd\")\n",
    "print(\"df = pd.read_csv('../outputs/results/feature_matrix.csv.gz', compression='gzip')\")\n",
    "print(f\"# Loads in ~{validation_results['csv_gz']['load_time']:.1f}s\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  Load Sample (Quick inspection):\")\n",
    "print(\"-\" * 80)\n",
    "print(\"import pandas as pd\")\n",
    "print(\"df_sample = pd.read_csv('../outputs/results/feature_matrix_sample.csv')\")\n",
    "print(f\"# Only 10,000 rows - loads in ~{validation_results['sample']['load_time']:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚ú® Optimization Complete!\")\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚Ä¢ Original file: {format_size(original_size_mb)}\")\n",
    "print(f\"  ‚Ä¢ Optimized Parquet: {format_size(parquet_size_mb)} ({parquet_compression_ratio:.1f}x compression)\")\n",
    "print(f\"  ‚Ä¢ Load time improvement: {load_time / validation_results['parquet']['load_time']:.1f}x faster\")\n",
    "print(f\"  ‚Ä¢ Potential disk savings: {format_size(total_saved)} ({(total_saved / original_size_mb * 100):.1f}%)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c70e8e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully optimized the 2GB feature_matrix.csv file by:\n",
    "\n",
    "‚úÖ Loading data efficiently in chunks to avoid memory issues  \n",
    "‚úÖ Optimizing data types (category, int32, float32) to reduce memory usage  \n",
    "‚úÖ Saving in Parquet format with ~5-10x compression and faster load times  \n",
    "‚úÖ Creating compressed CSV backup for universal compatibility  \n",
    "‚úÖ Generating sample CSV for quick inspection  \n",
    "‚úÖ Validating all outputs for data integrity  \n",
    "‚úÖ Creating detailed optimization report  \n",
    "\n",
    "**Next Steps:**\n",
    "1. Review the optimization report: `outputs/results/optimization_report.txt`\n",
    "2. Test loading the Parquet file in your analysis workflow\n",
    "3. After verification, delete the original 2GB CSV to free disk space\n",
    "4. Use the Parquet format for all future analysis (fastest and most efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d3cd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
